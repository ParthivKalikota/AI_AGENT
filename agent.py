# -*- coding: utf-8 -*-
"""Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EabGfTtaO0QJvN5FS3LJTMXLfMDut164
"""

pip install -qU langchain langchain-community langchain-core langchain-OpenAI langchain-nvidia-ai-endpoints python-dotenv langchain-milvus pypdf langchainhub langchain-tavily

from langchain_nvidia_ai_endpoints import ChatNVIDIA,NVIDIAEmbeddings
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from uuid import uuid4
from langchain_milvus import Milvus
from langchain.tools import Tool
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_core.prompts import PromptTemplate
from langchain_tavily import TavilySearch
import os
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
messages = [SystemMessage("")]
os.environ["NVIDIA_API_KEY"] = "API_KEY"
os.environ["TAVILY_API_KEY"] = "API_KEY"
SYSTEM_MESSAGE_CONTENT = """You are an AI assistant designed to provide accurate information based strictly and exclusively on the provided enterprise document excerpts and the ongoing conversation history.
Your responses must be derived solely from the information contained within the 'Context from enterprise documents' given for the current query, while also maintaining coherence with the 'Previous Conversation History'.
If the 'Context from enterprise documents' does not contain the necessary information to answer the 'Current Question', you must clearly state that the information is not available in the provided documents.
Do not use any external knowledge, make assumptions, or infer information beyond what is explicitly stated. Maintain a professional and objective tone.
Be precise and directly address the question."""
conversation_history = [
    SystemMessage(content=SYSTEM_MESSAGE_CONTENT)
]
MAX_HISTORY_MESSAGES = 10

embeddingModel = NVIDIAEmbeddings(model = "nvidia/llama-3.2-nv-embedqa-1b-v2")
LLM = ChatNVIDIA(model = "nvidia/llama-3.1-nemotron-70b-instruct")
milvus_uri = "./vectors.db"
vector_store = Milvus(
          embedding_function=embeddingModel,
          connection_args={"uri": milvus_uri},
          index_params={"index_type": "FLAT", "metric_type": "L2"},
          auto_id=True
      )

def ingest_pdf_to_milvus(pdf_file_path: str) -> str:
    """
    Loads a PDF from the given file path, splits it into chunks,
    embeds the chunks using NVIDIAEmbeddings, and stores them in a
    Milvus vector store.
    """
    try:
      print(f"Starting ingestion for: {pdf_file_path}")
      loader = PyPDFLoader(pdf_file_path)
      docs = loader.load()
      if not docs:
          return f"Error: Could not load any documents from {pdf_file_path}."
      print(f"Loaded {len(docs)} document(s) from PDF.")
      splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
      chunks = splitter.split_documents(docs)
      if not chunks:
          return f"Error: Could not split documents into chunks for {pdf_file_path}."
      print(f"Split into {len(chunks)} chunks.")
      print(f"Milvus vector store initialized with URI: {milvus_uri}")

      vector_store.add_documents(chunks)
      print(f"Successfully added {len(chunks)} chunks to Milvus.")

      return f"Successfully ingested {len(chunks)} chunks from {pdf_file_path} into Milvus."

    except Exception as e:
        return f"Error during ingestion of {pdf_file_path}: {str(e)}"
ingest_pdf_tool = Tool(
    name="IngestPDFToMilvus",
    func=ingest_pdf_to_milvus,
    description="Use this tool to load, chunk, embed, and store a PDF document into the Milvus vector store. Input must be the full file path to the PDF."
)

def query_from_database(question: str) -> str:
  """
    To get data or information from the vector store based on the query
  """
  try:
    if vector_store is None:
        error_msg = "Error: Vector store is not initialized or available."
        conversation_history.append(HumanMessage(content=question))
        conversation_history.append(AIMessage(content=error_msg))
        trim_history()
        return error_msg
    if LLM is None:
            error_msg = "Error: LLM instance is not initialized or available."
            conversation_history.append(HumanMessage(content=question))
            conversation_history.append(AIMessage(content=error_msg))
            trim_history()
            return error_msg
    retriever_from_llm = MultiQueryRetriever.from_llm(
      retriever=vector_store.as_retriever(),
      llm=LLM
    )
    retrieved_docs = retriever_from_llm.invoke(question)
    if not retrieved_docs:
            no_info_answer = "I couldn't find any relevant information in the enterprise documents for your current question."
            current_user_message = HumanMessage(content=question)
            current_ai_message = AIMessage(content=no_info_answer)
            conversation_history.extend([current_user_message, current_ai_message])
            trim_history()
            return no_info_answer
    context_text = "\n\n".join(doc.page_content for doc in retrieved_docs)
    messages_for_llm = []
    messages_for_llm.extend(conversation_history)
    prompt = PromptTemplate(
      template = """You are an AI assistant designed to provide accurate information and answer questions based strictly and exclusively on the provided enterprise document excerpts.
        Your responses must be derived solely from the information contained within the 'Context' given for each query.
        If the 'Context' does not contain the necessary information to answer a 'Question', you must clearly state that the information is not available in the provided documents or that you cannot answer based on the given context.
        Do not use any external knowledge, make assumptions, or infer information beyond what is explicitly stated in the 'Context'. Maintain a professional and objective tone.
        Be precise and directly address the user's question with the information found.

        Context:
        {context_text}

        Question: {question}

        Answer:""",
            input_variables=['context_text', 'question']
        )
    final_prompt = prompt.invoke({'context_text' : context_text, 'question' : question})
    messages_for_llm.append(HumanMessage(content = str(final_prompt)))
    answer = LLM.invoke(final_prompt)
    generated_answer = answer.content
    conversation_history.append(HumanMessage(content = question))
    conversation_history.append(AIMessage(content = generated_answer))
    return generated_answer
  except Exception as e:
        print(f"Error in query_from_database: {e}")
        return f"An error occurred while trying to answer your question from the database: {str(e)}"
def trim_history():
    """
    Trims the global conversation_history to MAX_HISTORY_MESSAGES,
    ensuring the SystemMessage remains if present.
    """
    global conversation_history, MAX_HISTORY_MESSAGES
    if MAX_HISTORY_MESSAGES is not None and len(conversation_history) > MAX_HISTORY_MESSAGES:
        # Check if the first message is a system message
        if conversation_history and conversation_history[0].type == "system":
            system_msg = conversation_history[0]
            relevant_user_ai_messages = conversation_history[-(MAX_HISTORY_MESSAGES - 1):]
            conversation_history = [system_msg] + relevant_user_ai_messages
        else:
            conversation_history = conversation_history[-MAX_HISTORY_MESSAGES:]
query_database_tool = Tool(
    name = "QueryVectorStore",
    func = query_from_database,
    description = "Use this tool to answer questions by retrieving relevant information from the college syllabus stored in the vector database and generating a response based on that information."
)

tool = TavilySearch(
    max_results=5,
    topic="general",
    include_answer=False,
    search_depth="advanced"
)

def tavily_search(question: str) -> str:
  """Use this tool If you did not find information from the database. This tool uses tavily to get information from the web."""
  result = tool.invoke({"query": question})
  return result
tavily_search_tool = Tool(
    name = "TavilySearch",
    func = tavily_search,
    description = "Use this tool to get information using tavily web search if You did not find information from the database only if the answer is a general question that can be found on the web."
)

from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser

from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser
from langchain.schema.runnable import RunnableParallel, RunnableLambda
class TopicMaker(BaseModel):
  topic1: str = Field(description="topic1")
  topic2: str = Field(description="topic2")
  topic3: str = Field(description="topic3")

def Report_Builder(Topic: str) -> str:
    """
    This Tool is used to generate a detailed Report on a given topic.
    It first generates 3 related sub-topics, gets information on each,
    and then merges the information into a final report using an LLM.
    """
    if LLM is None:
        return "Error: The LLM is not available for report generation."

    print(f"INFO: Report_Builder called for topic: {Topic}")

    try:
        pydanticParser = PydanticOutputParser(pydantic_object=TopicMaker)
        strParser = StrOutputParser()

        # Chain 1: Generate 5 related topics
        template1 = PromptTemplate(
            template='Generate 3 distinct and specific sub-topics that are related to the main topic: {topic_name}.\n{format_instruction}',
            input_variables=['topic_name'],
            partial_variables={'format_instruction': pydanticParser.get_format_instructions()}
        )
        primaryChain = template1 | LLM | pydanticParser
        print("INFO: primaryChain defined.")
        topic1Prompt = PromptTemplate(template='Provide a concise paragraph (around 50-70 words) of information on the following topic: {topic1_content}', input_variables=['topic1_content'])
        topic2Prompt = PromptTemplate(template='Provide a concise paragraph (around 50-70 words) of information on the following topic: {topic2_content}', input_variables=['topic2_content'])
        topic3Prompt = PromptTemplate(template='Provide a concise paragraph (around 50-70 words) of information on the following topic: {topic3_content}', input_variables=['topic3_content'])
        print("INFO: Individual topic prompts defined.")
        parallelChain = RunnableParallel({
            'topic1Info': topic1Prompt | LLM | strParser,
            'topic2Info': topic2Prompt | LLM | strParser,
            'topic3Info': topic3Prompt | LLM | strParser
        })
        print("INFO: parallelChain defined.")

        def format_topics_for_parallel_processing(topic_maker_output: TopicMaker) -> dict:
            print(f"INFO: Transforming TopicMaker output: {topic_maker_output}")
            if not isinstance(topic_maker_output, TopicMaker):
                print(f"WARNING: Expected TopicMaker object, got {type(topic_maker_output)}. Attempting to proceed if it's dict-like.")
                if isinstance(topic_maker_output, dict) and all(k in topic_maker_output for k in ["topic1", "topic2", "topic3"]):
                     return {
                        "topic1_content": topic_maker_output['topic1'],
                        "topic2_content": topic_maker_output['topic2'],
                        "topic3_content": topic_maker_output['topic3'],
                    }
                raise ValueError(f"Invalid input to transformation: Expected TopicMaker or parsable dict, got {type(topic_maker_output)}. Content: {topic_maker_output}")


            return {
                "topic1_content": topic_maker_output.topic1,
                "topic2_content": topic_maker_output.topic2,
                "topic3_content": topic_maker_output.topic3,
            }

        transformation_step = RunnableLambda(format_topics_for_parallel_processing)
        print("INFO: transformation_step defined.")

        mergePrompt = PromptTemplate(
            template="""Compile a detailed report based on the information provided for the following three sub-topics.
Present each sub-topic with its information clearly. The main topic was originally '{initial_topic}'.

Sub-topic 1 Information:
{topic1Info}

Sub-topic 2 Information:
{topic2Info}

Sub-topic 3 Information:
{topic3Info}


Ensure the final report is coherent, well-structured, and approximately 300 words in total.
Do not just list the information; synthesize it into a report format.
Start with a brief introduction related to the main topic '{initial_topic}' before detailing the sub-topics.
""",
            input_variables=['topic1Info', 'topic2Info', 'topic3Info', 'initial_topic']
        )
        mergeChain = mergePrompt | LLM | strParser
        print("INFO: mergeChain defined.")


        def prepare_input_for_merge_chain(parallel_output: dict, initial_topic: str) -> dict:

            final_input_for_merge = {**parallel_output, "initial_topic": initial_topic}
            print(f"INFO: Input for mergeChain: {final_input_for_merge.keys()}")
            return final_input_for_merge

        # Full chain construction
        # 1. primaryChain generates TopicMaker object
        # 2. transformation_step converts TopicMaker to dict for parallelChain's individual prompts
        # 3. parallelChain fetches info for each topic
        # 4. The result of parallelChain (dict of infos) and the original Topic are passed to mergeChain

        # We need to pass the original 'Topic' (as 'topic_name' from invoke) through to the merge chain
        # This can be done by ensuring the chain structure allows for it.

        # Let's define the sequence more explicitly:
        # Step 1: Generate topics
        # Input: {'topic_name': "Artificial Intelligence"}
        # Output of primaryChain: TopicMaker(topic1="...", topic2="...", ...)
        generated_topics = primaryChain

        # Step 2: Transform for parallel processing
        # Input: TopicMaker(...)
        # Output: {'topic1_content': "...", 'topic2_content': "...", ...}
        transformed_for_parallel = generated_topics | transformation_step

        # Step 3: Get info in parallel
        # Input: {'topic1_content': "...", ...}
        # Output: {'topic1Info': "...", 'topic2Info': "...", ...}
        parallel_info_gathering = transformed_for_parallel | parallelChain

        # Step 4: Prepare for merge and execute merge
        # We need to combine the output of parallel_info_gathering with the original 'Topic'
        # The 'chain.invoke({'topic_name': Topic})' will pass {'topic_name': Topic} as initial input.
        # We need to make sure 'topic_name' (or 'Topic') is available to the merge chain.

        # Using a lambda to combine the parallel output with the initial topic for the merge chain
        # The input to this lambda will be the output of parallel_info_gathering,
        # and we need to access the original 'Topic' from the initial invocation.
        # LCEL's context propagation can be tricky here.
        # A simpler way for this specific structure: invoke parts and pass results.
        # However, to keep it as one chain:

        final_chain = RunnableParallel(
            gathered_info=primaryChain | transformation_step | parallelChain, # This produces {'topic1Info': ..., etc.}
            initial_topic_passthrough=lambda x: x['topic_name'] # Passthrough the original topic name
        ) | RunnableLambda(lambda x: prepare_input_for_merge_chain(x['gathered_info'], x['initial_topic_passthrough'])) | mergeChain


        print("INFO: Full chain assembled.")

        # Invoke the chain
        # The initial input to the entire chain is {'topic_name': Topic}
        result = final_chain.invoke({'topic_name': Topic})
        print("INFO: Full chain invocation complete.")

        return result

    except Exception as e:
        print(f"ERROR in Report_Builder for topic '{Topic}': {str(e)}")
        # This will be the observation seen by the agent if the tool errors out
        return f"Error generating report for '{Topic}': {str(e)}. Please check logs."


Report_Builder_Tool = Tool(
    name='ReportBuilder',
    func=Report_Builder,
    description="""Use this tool to make a detailed report of a topic given by the user. Report should only be generated if user asks for it.
Example1 : Make a report on Artificial Intelligence
Example2 : Make a report on Agentic AI.
The report should be generated if the user's Query is similar to those of the given examples"""
)

from langchain.agents import create_react_agent, AgentExecutor
from langchain import hub

strict_instructions = """
YOU MUST FOLLOW THESE RULES FOR YOUR RESPONSE FORMAT VERY STRICTLY:

RULE 1: If you decide to use a tool, your response MUST exclusively contain ONLY the following three lines:
Thought: [Your brief thought process for choosing the action. Be concise.]
Action: [The name of the ONE tool you are using, from the available tools. Ensure the name is an exact match.]
Action Input: [The input string for the chosen tool.]
--- Your response MUST STOP exactly after the 'Action Input:' line. Do NOT add any other text, explanations, simulated observations, or any form of 'Final Answer'. ---

RULE 2: If you have the final answer for the user and do not need to use a tool, your response MUST exclusively contain ONLY the following two lines:
Thought: [Your brief thought process for arriving at the final answer. Be concise.]
Final Answer: [Your final answer to the user's original question.]
--- Your response MUST STOP exactly after the 'Final Answer:' line. Do NOT add any other text, explanations, or tool usage. ---

RULE 3: Do NOT simulate, explain, or generate the 'Observation:' field yourself. The actual 'Observation' (which is the result of the tool's execution) will be provided to you in a subsequent step if you used a tool. Wait for it.

RULE 4: Absolutely DO NOT include both an 'Action' section and a 'Final Answer' section in the same response. You must choose only ONE format (either Rule 1 or Rule 2) for each of your responses.

RULE 5: Be brief and stick to the defined structure. Do not add any conversational fluff or explanatory text beyond what is explicitly required by the 'Thought:', 'Action:', 'Action Input:', or 'Final Answer:' fields.

Adhere to these rules with extreme precision to ensure correct processing.
"""

original_prompt_obj = hub.pull('hwchase17/react')
original_template_str = original_prompt_obj.template

# Inject the strict instructions before the {agent_scratchpad}
# This placeholder is where the history of thoughts/actions/observations goes.
if "{agent_scratchpad}" in original_template_str:
    modified_template_str = original_template_str.replace(
        "{agent_scratchpad}",
        strict_instructions + "\n\n{agent_scratchpad}",
        1  # Replace only the first occurrence
    )
    print("INFO: Successfully injected strict instructions into the prompt template before {agent_scratchpad}.")
else:
    # Fallback if the placeholder isn't found (less ideal)
    modified_template_str = strict_instructions + "\n\n" + original_template_str
    print("WARNING: '{agent_scratchpad}' not found in the original prompt. Added instructions at the beginning. This might be less effective. Please inspect your original prompt structure.")



new_prompt_obj = PromptTemplate(
    template=modified_template_str,
    input_variables=original_prompt_obj.input_variables # Crucial: use the same input variables
)

tools = [ingest_pdf_tool, query_database_tool, tavily_search_tool, Report_Builder_Tool]

agent = create_react_agent(
    llm = LLM,
    tools = tools,
    prompt = new_prompt_obj
)

agent_executor = AgentExecutor(
    agent = agent,
    tools = tools,
    verbose = True
)

result = agent_executor.invoke({"input": "Upload the file in the path ./SampleData/Syllabus.pdf"})
print(result)

result = agent_executor.invoke({"input": "Give the syllabus of Design and Analysis of Algorithms Course"})
print(result)

print(result['output'])

result = agent_executor.invoke({"input": "What were the incidents happened in India last week?"})
print(result)

print(result['output'])

result = agent_executor.invoke({"input": "Make a detailed report of 300 words on Artificial Intelligence"})
print(result['output'])